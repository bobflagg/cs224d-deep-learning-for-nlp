# cs224d-deep-learning-for-nlp
Notes, code, and other resources for auditing the course [CS224d: Deep Learning for 
Natural Language Processing](http://cs224d.stanford.edu/).

### Readings
  - Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR, 2013.
  - Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean, Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.
  - [word2vec](http://www.reddit.com/r/MachineLearning/comments/30m0eb/what_is_the_state_of_the_art_in_language_modeling/)
  - [GloVe: Global Vectors for Word Representation](http://nlp.stanford.edu/projects/glove/)
  - [Quoc Le and Tomas Mikolov, Distributed Representations of Sentences and Documents](http://arxiv.org/pdf/1405.4053v2.pdf)
  - [Optimizing word2vec in gensim](http://radimrehurek.com/2013/09/word2vec-in-python-part-two-optimizing/)
  - [Scaling Recurrent Neural Network Language Models](http://arxiv.org/pdf/1502.00512v1.pdf), Will Williams, Niranjani Prasad, David Mrva, Tom Ash, Tony Robinson

